{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammad-rahbari/federated-learning_visual-classification/blob/main/notebooks/federated_model_visual_classification_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBf85ZRGj6HO",
        "outputId": "53347360-f32d-47e5-d9c6-dcc941f5e605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu0Iz0LDlRDK"
      },
      "source": [
        "# Importing DINO and installing its dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IckKkLcG0zeA",
        "outputId": "baa4b1da-56f2-40e0-c9db-2416928cdc19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dino'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Total 175 (delta 0), reused 0 (delta 0), pack-reused 175 (from 1)\u001b[K\n",
            "Receiving objects: 100% (175/175), 24.47 MiB | 15.81 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n"
          ]
        }
      ],
      "source": [
        "# @title Clon the DINO ripo\n",
        "!git clone https://github.com/facebookresearch/dino.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dY0yvkTfiNV3",
        "outputId": "93d043f2-b585-4ac1-eac1-5fc477955f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dino\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.16)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.6.15)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# @title Installing required dependencies regarding DINO\n",
        "%cd dino\n",
        "!pip install -r requirements.txt\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9hv0ik3jZ-8"
      },
      "source": [
        "\n",
        "# preprocessing the CIFAR-100 dataset\n",
        "\n",
        "feature size in CIFAR is 32x32 but DINO requires 224x224 in the input layer.\n",
        "\n",
        "In first step we upscale the dataset and then we add randomization to it\n",
        "\n",
        "In last step of transformation we normalize data usind mean value and standard division of ImageNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dDodXJD_lPCd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split,DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_4ydT67FmAQR"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNgGMkqaqX-G",
        "outputId": "5807563c-a48b-4229-db13-382f5e3719fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:18<00:00, 9.26MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of combined dataset: 50000\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import ConcatDataset\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR100\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "full_train = train_dataset\n",
        "\n",
        "# Verify the length of the new dataset\n",
        "print(f\"Length of combined dataset: {len(full_train)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dNQ67lu7cYNj"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Imports\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4oXrqMHwAit"
      },
      "source": [
        "# Set Hyperparameters regarding the data spliting here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VJfqNlD_ORqk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title set the parameters here!!\n",
        "\n",
        "\n",
        "\n",
        "number_of_clients = None\n",
        "train_frac = 0.8 #@param\n",
        "val_frac = 0.2 #@param\n",
        "batch_size = 32 #@param{type:\"integer\"}\n",
        "is_seed_fixed = True #@param{type:\"boolean\"}\n",
        "seed = 42 #@param{type:\"integer\"}\n",
        "\n",
        "def set_seed(seed=42, is_seed_fixed=True):\n",
        "  if not is_seed_fixed:\n",
        "    return\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(seed,is_seed_fixed)\n",
        "\n",
        "\n",
        "\n",
        "#@markdown </br> <h5>Indicate the number of clients that contribute in training:</h5>\n",
        "n_clients = 80 #@param{type:\"integer\"}\n",
        "\n",
        "#@markdown </br></br> <b>splitting hyperparameters</b>\n",
        "\n",
        "spliting_method = \"non-i.i.d. sharing\" #@param[\"i.i.d. sharing\",\"non-i.i.d. sharing\"]\n",
        "backbone = \"dino_vits16\" #@param[\"dino_resnet50\", \"dino_vits16\", \"dino_xcit_small_12_p16\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W4gR7IvnxqL6"
      },
      "outputs": [],
      "source": [
        "#@title Set the parameters here only if <b>non-i.i.d. sharing</b> method had been selected!!\n",
        "#@markdown Nc is the number of classes that each subset can contain\n",
        "if spliting_method == \"non-i.i.d. sharing\":\n",
        "  Nc = 25 #@param{type:\"integer\"}\n",
        "\n",
        "  # are_classes_overlaping = False #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown <h3>If we consider the Number of classes M and nummber of client K then:</h3>\n",
        "#@markdown <ul>\n",
        "#@markdown   <li>Nc should be:\n",
        "#@markdown     <ul>\n",
        "#@markdown       <li>\n",
        "#@markdown         Greater than or equal to <b>\\\\(\\frac{M}{K}\\\\)</b>\n",
        "#@markdown       </li>\n",
        "#@markdown       <li>\n",
        "#@markdown         Less than or equal to K </b>\n",
        "#@markdown       </li>\n",
        "#@markdown     </ul>\n",
        "#@markdown   </li>\n",
        "#@markdown   <li>\n",
        "#@markdown   Muximum number of clients means all classes contribute in every client\n",
        "#@markdown   </li>\n",
        "\n",
        "#@markdown </ul>\n",
        "\n",
        "\n",
        "#@markdown </br></br><h3>Combination of classes are randomly selected which suits definition of federated learning especially Cross-device federated learning</h3>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVtH-qLIrAeh"
      },
      "source": [
        "# Data splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmFcWGtXrLS1",
        "outputId": "c7791932-d516-4f3a-fb18-ead62f7d2f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 40000\n",
            "Validation dataset size: 10000\n",
            "Size of subset:  [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]\n"
          ]
        }
      ],
      "source": [
        "# @title data splitting\n",
        "\n",
        "set_seed(seed,is_seed_fixed)\n",
        "generator = torch.Generator().manual_seed(seed)\n",
        "\n",
        "total_size = len(full_train)\n",
        "train_size = int(train_frac * total_size)\n",
        "val_size   = total_size - train_size\n",
        "\n",
        "train_set, val_set = random_split(full_train, [train_size, val_size], generator=generator)\n",
        "train_indices = torch.tensor(train_set.indices)\n",
        "val_indices = torch.tensor(val_set.indices)\n",
        "\n",
        "train_set = Subset(train_set.dataset, train_indices)\n",
        "val_set = Subset(val_set.dataset, val_set.indices)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=len(train_set), shuffle=False)\n",
        "val_loader  =  DataLoader(val_set, batch_size=len(val_set), shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Train dataset size: {len(train_set)}\")\n",
        "print(f\"Validation dataset size: {len(val_set)}\")\n",
        "\n",
        "lenghts = [train_size//n_clients] * n_clients\n",
        "\n",
        "for i in range(train_size % n_clients):\n",
        "  lenghts[i] += 1\n",
        "print(\"Size of subset: \", lenghts)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n86IvSfkp9Zv"
      },
      "outputs": [],
      "source": [
        "# @title i.i.d sharing - split data dased on number of clients and with respect of label proportionality\n",
        "set_seed(seed,is_seed_fixed)\n",
        "def iid_sharing(dataset, n_clients):\n",
        "\n",
        "  full_train_indices = dataset.indices\n",
        "  full_train_labels = torch.from_numpy(np.array(dataset.dataset.targets)[full_train_indices]) #collects labels from all dataset\n",
        "  unique_lables = torch.unique(full_train_labels) #Removes dupilication and generates a uniuqe list of labels (classes)\n",
        "  proportionality ={}\n",
        "  classes_indices = {}\n",
        "\n",
        "\n",
        "  for i in unique_lables:\n",
        "    proportionality[i] =( full_train_labels == i).sum() / len(full_train_labels) #Calculates proportinality of each class\n",
        "    classes_indices[i] = torch.nonzero(full_train_labels == i).squeeze() #Collects and save Indices in an array based on classes\n",
        "\n",
        "  for i in classes_indices.keys():\n",
        "    classes_indices[i] = classes_indices[i][torch.randperm(classes_indices[i].shape[0])] #suffels the indices\n",
        "\n",
        "  client_data_size = len(full_train_labels) / n_clients #Minimum dataset size of each client\n",
        "\n",
        "  client_indices = {}\n",
        "\n",
        "  #For each client we generate a element in client_indices dict to keep track of indices we'll associated with each client\n",
        "  for client in range(n_clients):\n",
        "    if not client_indices.get(client):\n",
        "      client_indices[client] = torch.empty(0, dtype=torch.long)\n",
        "  #__________________\n",
        "\n",
        "\n",
        "  #For each client we calculate how many samples from each specific label should be seperated. We take out the requried number of them form the list\n",
        "    for label in proportionality.keys():\n",
        "      pointer = proportionality[label] * client_data_size\n",
        "      pointer = int(pointer) if not pointer % 1 else int(pointer) + 1\n",
        "      pointer = min(pointer,classes_indices[label].size()[0])\n",
        "      pointer = pointer if pointer < classes_indices[label].size()[0] else classes_indices[label].size()[0]\n",
        "      client_indices[client] = torch.cat((client_indices[client], classes_indices[label][:pointer]), dim=0)\n",
        "      classes_indices[label] = classes_indices[label][pointer:]\n",
        "\n",
        "\n",
        "  #After spliting data we distribute remaining samples amoung the clients\n",
        "  for label in classes_indices.keys():\n",
        "    while True:\n",
        "      for client in client_indices.keys():\n",
        "\n",
        "        if classes_indices[label].size()[0] == 0:\n",
        "          break\n",
        "        client_indices[client] = torch.cat((\n",
        "            client_indices[client],\n",
        "            classes_indices[label][:1] ),\n",
        "            dim=0)\n",
        "\n",
        "\n",
        "        classes_indices[label] = classes_indices[label][1:]\n",
        "\n",
        "      if classes_indices[label].size()[0] == 0:\n",
        "        break\n",
        "\n",
        "\n",
        "  #split actual dataset to multiple subset for clients\n",
        "  client_data={\n",
        "      client_id: Subset(dataset.dataset,indices[torch.randperm(len(indices))])\n",
        "      for client_id, indices in client_indices.items()\n",
        "  }\n",
        "  return client_data\n",
        "\n",
        "# indices_check = []\n",
        "# client_data = iid_sharing(train_set, n_clients)\n",
        "# for client_id in client_data.keys():\n",
        "#   indices_check = indices_check + list(client_data[client_id].indices)\n",
        "#   print(f\"Client {client_id} has {len(client_data[client_id])} samples\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UHYGEz82-ZJs"
      },
      "outputs": [],
      "source": [
        "# @title Non i.i.d sharing\n",
        "\n",
        "\n",
        "# @title i.i.d sharing - split data dased on number of clients and with respect of label proportionality\n",
        "set_seed(seed,is_seed_fixed)\n",
        "def noniid_sharing(dataset,Nc , n_clients):\n",
        "\n",
        "  full_train_indices = dataset.indices\n",
        "  full_train_labels = torch.tensor(dataset.dataset.targets)[full_train_indices] #collects labels from all dataset\n",
        "  unique_lables = torch.unique(full_train_labels) #Removes dupilication and generates a uniuqe list of labels (classes)\n",
        "\n",
        "  classes_indices = {}\n",
        "  classes_size = torch.zeros(unique_lables.size()[0])\n",
        "\n",
        "  class_combs = get_class_combinations(unique_lables, Nc, n_clients)\n",
        "\n",
        "  classes_num_partition = torch.zeros(unique_lables.size()[0])\n",
        "\n",
        "  for i in unique_lables:\n",
        "    classes_num_partition[i] = torch.sum(class_combs == i)\n",
        "    classes_indices[i.item()] = torch.nonzero(full_train_labels == i).squeeze() #Collects and save Indices in an array based on classe\n",
        "    classes_size[i] = classes_indices[i.item()].size()[0] #Calculate the number of smaples belonging to each class\n",
        "\n",
        "  for i in classes_indices.keys():\n",
        "    classes_indices[i] = classes_indices[i][torch.randperm(classes_indices[i].shape[0])] #suffels the indices\n",
        "\n",
        "  client_indices = {client: torch.tensor([],dtype=torch.int64) for client in range(n_clients) }\n",
        "  assigned_indices = set()\n",
        "\n",
        "  #For each client we generate a element in client_indices dict to keep track of indices we'll associated with each client\n",
        "\n",
        "  for client in range(n_clients):\n",
        "    for cls in class_combs[client]:\n",
        "      cls = cls.item()\n",
        "\n",
        "      portion  = classes_size[cls] /classes_num_partition[cls]\n",
        "      portion = int(portion) if not portion % 1 else int(portion) + 1\n",
        "      portion = min(portion, classes_indices[cls].size()[0])\n",
        "\n",
        "      class_partition = classes_indices[cls][:portion]\n",
        "\n",
        "      class_partition = [idx for idx in class_partition if idx not in assigned_indices]\n",
        "\n",
        "      assigned_indices.update(class_partition)\n",
        "\n",
        "      class_partition = torch.tensor(class_partition, dtype=torch.int64)\n",
        "\n",
        "      client_indices[client] = torch.cat((client_indices[client], class_partition), dim=0)\n",
        "\n",
        "      classes_indices[cls] = classes_indices[cls][portion:]\n",
        "\n",
        "  client_data={\n",
        "      client_id: Subset(dataset.dataset,indices[torch.randperm(len(indices))])\n",
        "      for client_id, indices in client_indices.items()\n",
        "      if len(indices) > 0\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "  return client_data, class_combs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_class_combinations(classes, Nc, n_clients):\n",
        "\n",
        "  if Nc * n_clients < len(classes):\n",
        "    Nc = len(classes) / n_clients\n",
        "    Nc = int(Nc) if not Nc % 1 else int(Nc) + 1\n",
        "\n",
        "    print(f\"Number of classes per clients is lower then minimum. Nc changed to {Nc} (the least possible value)\")\n",
        "\n",
        "  combinations = torch.zeros((n_clients,Nc),dtype= torch.int64)\n",
        "  counter =0\n",
        "  ofset = 0\n",
        "  flag = False\n",
        "\n",
        "  for i in range(n_clients):\n",
        "    if not flag:\n",
        "      end_pointer = (i + 1) * Nc\n",
        "      if end_pointer >= classes.size()[0]:\n",
        "          ofset = (end_pointer - classes.size()[0])\n",
        "          flag = True\n",
        "\n",
        "      combinations[i] = classes[i* Nc - ofset: end_pointer - ofset]\n",
        "\n",
        "    else:\n",
        "\n",
        "      combinations[i]  = torch.randperm(classes.size()[0])[:Nc]\n",
        "\n",
        "  return combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqWviCIKs-eN"
      },
      "source": [
        "# Log System\n",
        "\n",
        "In this section Requerd Data will be stored.<br/><br/>\n",
        "**Archaving this information will make it possible to:**\n",
        "*   Handle Clients\n",
        "*   Manage the models\n",
        "*   Keep track of results of different Backbones\n",
        "*   Compare measurement criteria\n",
        "*   Handel model merging process\n",
        "*   Save path to the models\n",
        "\n",
        "<br/><br/>\n",
        "**These data will be saved in two seperted csv file to :**\n",
        "\n",
        "1.   Store the LOCAL Models  \n",
        "2.   Store the GLOBAL Models resulted by each round\n",
        "\n",
        "<br/><br/>\n",
        "The csv files will be handeled as panda.dataframe and each row in the csv file addresses one of models\n",
        "<br/>\n",
        "\n",
        "**Columns (COMMON):**<br/>\n",
        "1. Backbone model name\n",
        "2. Model name\n",
        "3. Path\n",
        "4. Time of log\n",
        "5. Measurement criteria\n",
        " * loss\n",
        " * Accuracy\n",
        " * ...?\n",
        "6. Size of dataset\n",
        "\n",
        "**Columns (Local Models only):**<br/>\n",
        "7. Client Id\n",
        "8. Classes (Indicate which classes have been covered by each client)(format:\"2,4,63,80,9\" or \"all\" for all the classes)\n",
        "9. Round number\n",
        "10. Duration of training\n",
        "11. Train Test ratio\n",
        "\n",
        "**Columns (Global Models only):**<br/>\n",
        "7. Number of clients\n",
        "7. Number of rounds\n",
        "8. Model Aggregation method\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CasHI07Ps4A8"
      },
      "outputs": [],
      "source": [
        "# @title Functions\n",
        "import torch\n",
        "from datetime import datetime\n",
        "import time\n",
        "from google.colab import drive\n",
        "from uuid import uuid4\n",
        "import os\n",
        "\n",
        "def get_current_time():\n",
        "  now = datetime.now()\n",
        "\n",
        "  formatted_date_time = now.strftime(\"%Y-%m-%d %H:%M:%S\") # Format the date and time as a string\n",
        "\n",
        "  return formatted_date_time\n",
        "\n",
        "\n",
        "\n",
        "tic_start_time = None\n",
        "\n",
        "def next_id(log_path):\n",
        "  if os.path.exists(log_path):\n",
        "    df = pd.read_csv(log_path)\n",
        "    while True:\n",
        "      uuid = str(uuid4())\n",
        "      if uuid not in df[\"model_name\"].values:\n",
        "        return uuid\n",
        "  else:\n",
        "    return str(uuid4())\n",
        "\n",
        "\n",
        "\n",
        "def tic():\n",
        "    global tic_start_time\n",
        "    tic_start_time = time.perf_counter() # start the timer\n",
        "\n",
        "def toc():\n",
        "    if tic_start_time is None:\n",
        "        print(\"Error: You must call tic() before toc()\")\n",
        "        return None\n",
        "    elapsed_time = time.perf_counter() - tic_start_time\n",
        "    return elapsed_time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l66ikAyqfg4f"
      },
      "source": [
        "# Model and model configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iiwRCHrwfk63"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "\n",
        "class DinoClassifier(nn.Module):\n",
        "  def __init__(self, dino_model, num_classes:int=100, device=None):\n",
        "    super(DinoClassifier, self).__init__()\n",
        "    self.backbone = dino_model\n",
        "\n",
        "    #We need to freaze thhe parameters of bakbone first so we can train only on the head layer(output layer)\n",
        "    for param in self.backbone.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    #determine the Device\n",
        "    if device is None:\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    self.backbone.to(device)\n",
        "\n",
        "    #To detect the output feature dimontion of backbone we run  Dummy forward pass\n",
        "    with torch.no_grad():\n",
        "\n",
        "      dummy_input = torch.randn(1,3,224,224).to(device)\n",
        "      dummy_out = self.backbone(dummy_input)\n",
        "\n",
        "\n",
        "      if isinstance(dummy_out, tuple):\n",
        "        dummy_out = dummy_out[0]\n",
        "      elif isinstance(dummy_out, dict):\n",
        "        dummy_out = dummy_out.get(\"x_norm_clstoken\", next(iter(dummy_out.values())))\n",
        "\n",
        "      #If the output is 3D (B, T, D), we assume first token is the [CLS] token.\n",
        "      if dummy_out.dim() == 3:\n",
        "        dummy_feature = dummy_out[:,0]\n",
        "      else:\n",
        "        dummy_feature = dummy_out\n",
        "      feature_dim = dummy_feature.shape[1]\n",
        "      print(\"Detected feature dimontion:\", feature_dim)\n",
        "\n",
        "\n",
        "      #Hidden Layer\n",
        "      self.hidden = nn.Sequential(\n",
        "          nn.Linear(feature_dim, 128),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "\n",
        "\n",
        "      #Difineing the classification Head\n",
        "      self.head = nn.Linear(128, num_classes)\n",
        "\n",
        "      #Ensure the head is trainable.\n",
        "      for param in self.hidden.parameters():\n",
        "        param.requires_grad = True\n",
        "      for param in self.head.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    #pass the input through the backbone\n",
        "    features = self.backbone(x)\n",
        "\n",
        "    if isinstance(features, tuple):\n",
        "      features = features[0]\n",
        "    elif isinstance(features, dict):\n",
        "      features = features.get(\"x_norm_clstoken\", next(iter(features.values())))\n",
        "\n",
        "\n",
        "    # If featers are retuened as (B, T, D), use the first token\n",
        "    if features.dim() == 3:\n",
        "      features = features[:,0]\n",
        "\n",
        "\n",
        "    hidden_out  = self.hidden(features)\n",
        "\n",
        "    logits = self.head(hidden_out)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def linear_probe_eval(self, train_set, device, accuracy_threshold=30.0):\n",
        "      \"\"\"Train a linear probe on top of frozen encoder, return per-class accuracy.\"\"\"\n",
        "\n",
        "      self.eval()\n",
        "      # Freeze the backbone (not the head)\n",
        "      for param in self.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "      for param in self.hidden.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "      # Linear probe\n",
        "      embedding_dim = self.head.in_features\n",
        "      num_classes = self.head.out_features\n",
        "      linear_probe = nn.Linear(embedding_dim, num_classes).to(device)\n",
        "\n",
        "      optimizer = optim.SGD(linear_probe.parameters(), lr=1e-3)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      # Probing training\n",
        "      epochs = 5\n",
        "      for epoch in range(epochs):\n",
        "          linear_probe.train()\n",
        "          for x, y in train_set:\n",
        "              x, y = x.to(device), y.to(device)\n",
        "              with torch.no_grad():\n",
        "                features = self.backbone(x) # Changed from self.encoder to self.backbone\n",
        "                if isinstance(features, tuple):\n",
        "                  features = features[0]\n",
        "                elif isinstance(features, dict):\n",
        "                  features = features.get(\"x_norm_clstoken\", next(iter(features.values())))\n",
        "\n",
        "                if features.dim() == 3:\n",
        "                  features = features[:, 0]\n",
        "\n",
        "                hidden_out = self.hidden(features)\n",
        "\n",
        "              logits = linear_probe(hidden_out)\n",
        "              loss = criterion(logits, y)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "      class_correct = defaultdict(int)\n",
        "      class_total = defaultdict(int)\n",
        "      linear_probe.eval()\n",
        "      with torch.no_grad():\n",
        "          for x, y in train_set:\n",
        "              x, y = x.to(device), y.to(device)\n",
        "              features = self.backbone(x) # Changed from self.encoder to self.backbone\n",
        "              if isinstance(features, tuple):\n",
        "                features = features[0]\n",
        "              elif isinstance(features, dict):\n",
        "                features = features.get(\"x_norm_clstoken\", next(iter(features.values())))\n",
        "\n",
        "              if features.dim() == 3:\n",
        "                features = features[:, 0]\n",
        "\n",
        "              hidden_out = self.hidden(features)\n",
        "              logits = linear_probe(hidden_out)\n",
        "              _, preds = torch.max(logits, 1)\n",
        "              for true, pred in zip(y, preds):\n",
        "                  class_total[int(true)] += 1\n",
        "                  if int(true) == int(pred):\n",
        "                      class_correct[int(true)] += 1\n",
        "\n",
        "      class_accuracy = {\n",
        "          c: 100 * class_correct[c] / class_total[c]\n",
        "          for c in class_total\n",
        "      }\n",
        "\n",
        "      weak_classes = [c for c, acc in class_accuracy.items() if acc < accuracy_threshold]\n",
        "\n",
        "      # print(f\"Client {self.id} - Weak classes detected: {weak_classes}\") # Removed client id here\n",
        "      return class_accuracy, weak_classes\n",
        "\n",
        "  def head_only(self):\n",
        "      for param in self.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "      for param in self.hidden.parameters():\n",
        "        param.requires_grad = False\n",
        "      for param in self.head.parameters():\n",
        "        param.requires_grad = True\n",
        "  def full_train(self):\n",
        "      for param in self.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "      for param in self.hidden.parameters():\n",
        "        param.requires_grad = True\n",
        "      for param in self.head.parameters():\n",
        "        param.requires_grad = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9teqJtq_fG47"
      },
      "source": [
        "# Clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2ynAlV1-oyhY"
      },
      "outputs": [],
      "source": [
        "from warnings import filters\n",
        "#@title clients Classs\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Subset\n",
        "import pandas as pd\n",
        "import torch.hub\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import DataLoader # Import DataLoader\n",
        "\n",
        "set_seed(seed,is_seed_fixed)\n",
        "\n",
        "class Client:\n",
        "\n",
        "\n",
        "  def __init__(self, id, data, n_clients, spliting_method, grad_mask = True, sparsity=0.4, batch_size = 32, classes=\"all\", num_epochs= 10, backbone=None, path_to_model=None, initial_model=None, spliting_ratio={\"train\":0.8, \"test\":0.2}, path_to_subsets=\"\", path_to_class_combs=\"\"):\n",
        "    self.id = id\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Corrected cuda() to is_available()\n",
        "    self.data_set = data\n",
        "    self.weak_set = None\n",
        "    self.weak_classes = None\n",
        "    self.spliting_method = spliting_method\n",
        "    self.classes = classes\n",
        "    self.backbone = backbone\n",
        "    self.sparsity = sparsity\n",
        "    self.grad_mask = grad_mask\n",
        "    self.path_to_model = path_to_model\n",
        "    self.n_clients = n_clients\n",
        "    if initial_model:\n",
        "      self.model = initial_model\n",
        "    else:\n",
        "      self.model = None\n",
        "    self.load_model()\n",
        "    self.num_epochs = num_epochs\n",
        "    self.num_edit_epochs = None\n",
        "    self.spliting_ratio = spliting_ratio\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.train_set , self.test_set = self.test_train_split()\n",
        "    self.duration = 0.0\n",
        "    self.train_loss = None\n",
        "    self.accuracy = None\n",
        "    self.loss = None\n",
        "    self.path_to_subsets = path_to_subsets\n",
        "    self.path_to_class_combs = path_to_class_combs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def test_train_split(self):\n",
        "    train_size = int(self.spliting_ratio.get(\"train\") * len(self.data_set))\n",
        "    test_size =  len(self.data_set) - train_size\n",
        "\n",
        "    train_set, test_set = random_split(self.data_set, [ train_size, test_size ])\n",
        "    train_set = DataLoader(train_set, batch_size=self.batch_size, shuffle=True,  num_workers=2)\n",
        "    test_set = DataLoader(test_set, batch_size=self.batch_size, shuffle=False,  num_workers=2)\n",
        "\n",
        "    return train_set, test_set\n",
        "\n",
        "\n",
        "  def weak_set_generator(self,weak_classes):\n",
        "    filtered_indices = [i for i,(_,label) in enumerate(self.train_set.dataset) if label in weak_classes]\n",
        "    filtered_train_set = Subset(train_set, filtered_indices)\n",
        "    self.weak_set = DataLoader(filtered_train_set, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "  def gradient_mask(self):\n",
        "\n",
        "    masks = {}\n",
        "    all_grads = []\n",
        "\n",
        "\n",
        "    for name, param in self.model.head.named_parameters():\n",
        "      if param.grad is not None and param.requires_grad and 'weight' in name:\n",
        "        all_grads.append((param.grad **2).flatten())\n",
        "\n",
        "    all_grads_flat = torch.cat(all_grads)\n",
        "    k = int(all_grads_flat.numel() * self.sparsity)\n",
        "    threshold = torch.topk(all_grads_flat, k, largest=False).values.max() if k > 0 else 0.0\n",
        "\n",
        "    for name, param in self.model.head.named_parameters():\n",
        "      if param.grad is not None and param.requires_grad and 'weight' in name:\n",
        "        masks[name] = ((param.grad**2) > threshold).float()\n",
        "\n",
        "    for name, param in self.model.head.named_parameters():\n",
        "        if name in masks and param.grad is not None:\n",
        "\n",
        "          param.grad *= masks[name].to(param.grad.device)\n",
        "\n",
        "\n",
        "    for name, param in self.model.hidden.named_parameters():\n",
        "      if param.grad is not None and param.requires_grad and 'weight' in name:\n",
        "        all_grads.append((param.grad **2).flatten())\n",
        "\n",
        "    all_grads_flat = torch.cat(all_grads)\n",
        "    k = int(all_grads_flat.numel() * self.sparsity)\n",
        "    threshold = torch.topk(all_grads_flat, k, largest=False).values.max() if k > 0 else 0.0\n",
        "\n",
        "    for name, param in self.model.hidden.named_parameters():\n",
        "      if param.grad is not None and param.requires_grad and 'weight' in name:\n",
        "        masks[name] = ((param.grad**2) > threshold).float()\n",
        "\n",
        "    for name, param in self.model.hidden.named_parameters():\n",
        "        if name in masks and param.grad is not None:\n",
        "\n",
        "          param.grad *= masks[name].to(param.grad.device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def load_model(self):\n",
        "\n",
        "    if self.path_to_model:\n",
        "      dino_model = torch.hub.load('facebookresearch/dino:main', self.backbone)\n",
        "      self.model = DinoClassifier(dino_model=dino_model, num_classes=100, device=self.device)\n",
        "      state_dict = torch.load(self.path_to_model)\n",
        "      self.model.load_state_dict(state_dict)\n",
        "\n",
        "    self.model.to(self.device)\n",
        "\n",
        "\n",
        "  def train(self, model_edit:bool = False):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(self.model.head.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "\n",
        "    tic()\n",
        "\n",
        "    for epoch in range(self.num_epochs if not model_edit else self.num_edit_epochs):\n",
        "      self.model.train()\n",
        "\n",
        "      if model_edit:\n",
        "        self.model.head_only()\n",
        "        train_data = self.weak_set\n",
        "\n",
        "      else:\n",
        "        self.model.full_train()\n",
        "        train_data = self.train_set\n",
        "\n",
        "      running_loss = 0.0\n",
        "\n",
        "\n",
        "      for index, (images, labels) in enumerate(train_data):\n",
        "        images = images.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = self.model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if self.grad_mask:\n",
        "          self.gradient_mask()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "      epoch_loss = running_loss / len(self.train_set)\n",
        "      status = \"Training\" if not model_edit else \"Model editing\"\n",
        "      print(f\"client {self.id}, {status}- epoch {epoch} - epoch loss:{epoch_loss:.4f}\" )\n",
        "      self.duration = toc()\n",
        "      self.train_loss = epoch_loss\n",
        "\n",
        "\n",
        "  def evaluate(self):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    self.model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for index, (images, labels) in enumerate(self.train_set):\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        outputs = self.model(images)\n",
        "\n",
        "        _, prediction = torch.max(outputs.data,1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * labels.size(0)\n",
        "\n",
        "\n",
        "        total += labels.size(0)\n",
        "\n",
        "        correct += (prediction == labels).sum().item()\n",
        "    self.accuracy = 100 * correct / total\n",
        "    self.loss = test_loss / total\n",
        "\n",
        "  def model_edit(self,weak_classes, num_epochs:int=5):\n",
        "    self.weak_set_generator(weak_classes)\n",
        "    self.weak_classes = weak_classes\n",
        "    self.num_edit_epochs = num_epochs\n",
        "    self.train(model_edit=True)\n",
        "\n",
        "\n",
        "  def confirm_save(self,path):\n",
        "      # torch.save(self.model.state_dict(),  path ) # saves whole model\n",
        "      torch.save({\n",
        "          'hidden': self.model.hidden.state_dict(),\n",
        "          'head' : self.model.head.state_dict()\n",
        "      },path)\n",
        "\n",
        "  def create_log(self, model_name, path, round_number):\n",
        "\n",
        "    log_dict= {\n",
        "        \"client_id\":[self.id],\n",
        "        \"backbone\":[self.backbone],\n",
        "        \"model_name\":[model_name],\n",
        "        \"initial_model_name\":[initial_model_name],\n",
        "        \"path\": [path],\n",
        "        \"num_of_clients\":[self.n_clients],\n",
        "        \"Measurement_criteria\":[\"accuracy,loss,train_loss\"],\n",
        "        \"accuracy\":[self.accuracy],\n",
        "        \"loss\":[self.loss],\n",
        "        \"train_loss\":[self.train_loss],\n",
        "        \"splitting_method\":[self.spliting_method],\n",
        "        \"gradient_mask\":[self.grad_mask],\n",
        "        \"sparsity\":[self.sparsity],\n",
        "        \"size_of_dataset\": [len(self.data_set.dataset)],\n",
        "        \"client_train_size\":[len(self.train_set.dataset)],\n",
        "        \"client_test_size\":[len(self.test_set.dataset)],\n",
        "        \"train_test_ratio\":[self.spliting_ratio],\n",
        "        \"classes\":[self.classes],\n",
        "        \"round_number\":[round_number],\n",
        "        \"duration\":[self.duration],\n",
        "        \"time\": [get_current_time()],\n",
        "        \"path_to_subsets\":[self.path_to_subsets],\n",
        "        \"path_to_class_combs\":[self.path_to_class_combs],\n",
        "        \"has_model_edited\":[not self.weak_set is None],\n",
        "        \"weak_classes\":[self.weak_classes]\n",
        "    }\n",
        "\n",
        "\n",
        "    return pd.DataFrame(log_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>❗ Important Notice ❗</h1>**\n",
        "\n",
        "**Regarding `save_data`:**\n",
        "Please be aware that checking the `save_data` option will generate a **new data subset** and a **new initial model** based on your specified parameters.\n",
        "\n",
        "**⚠️ Crucial: Using Existing Models with New Data/Parameters ⚠️**\n",
        "If you intend to use an *existing model* but wish to apply it to a *different data subset*, use a *different data splitting method*, or make *any other changes to the data or algorithm*, you **MUST** assign a **new and unique model name**.\n",
        "\n",
        "**Why is this critical?**\n",
        "Failing to use a unique model name will make it impossible to differentiate between models for each client when filtering. This will lead to inaccurate results from the client aggregation function on the server."
      ],
      "metadata": {
        "id": "aw0_IYcv1m7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title using this block we can duplicate a spicific global model\n",
        "duplicate_model = False\n",
        "import shutil\n",
        "if duplicate_model:\n",
        "  d_model_name = \"3b568f90-5869-4a6a-bddb-68659dd23018\"\n",
        "  temp_df = pd.read_csv('/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv')\n",
        "  duplicated_recored = temp_df[temp_df['model_name']== d_model_name].copy()\n",
        "  print(duplicated_recored['model_name'].values[0])\n",
        "  print(duplicated_recored['path'].values[0])\n",
        "\n",
        "\n",
        "  duplicated_recored[\"model_name\"] = [next_id('/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv')]\n",
        "  duplicated_recored['path'] = [duplicated_recored['path'].values[0].replace(d_model_name,duplicated_recored[\"model_name\"].values[0])]\n",
        "  print(duplicated_recored['model_name'].values[0])\n",
        "  print(duplicated_recored['path'].values[0])\n",
        "  shutil.copy(temp_df[temp_df['model_name']== d_model_name]['path'].values[0], duplicated_recored['path'].values[0])\n",
        "  duplicated_recored.to_csv('/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv', mode='a', header=False, index=False)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mA0UoUu65h4D",
        "outputId": "6c79c3c7-fb9c-49de-ea1b-74b6a15d8379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fa4e78ad-aca5-491e-8d8c-9e366f73a708\n",
            "/content/drive/MyDrive/MLDL_FederatedLearning/models/global/fa4e78ad-aca5-491e-8d8c-9e366f73a708.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "load_data = True #@param{\"type\":\"boolean\"}\n",
        "initial_model_name = \"b8437215-ee1e-48a4-8a5c-1fbfabaf6dcb\" #@param{\"type\":\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if load_data:\n",
        "\n",
        "  initial_model_log_df = pd.read_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\")\n",
        "  prev_global_model_name = initial_model_log_df[initial_model_log_df[\"model_name\"] == initial_model_name][\"prev_global_model_name\"].values[0]\n",
        "  prev_clients = pd.read_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\")\n",
        "  prev_clients = prev_clients[prev_clients[\"initial_model_name\"]== prev_global_model_name];\n",
        "\n",
        "  path_to_subsets =prev_clients[\"path_to_subsets\"].values[0]\n",
        "  path_to_class_combs = prev_clients[\"path_to_class_combs\"].values[0]\n",
        "  print(path_to_subsets)\n",
        "  print(path_to_class_combs)\n",
        "\n",
        "\n",
        "  initial_model_log_df = pd.read_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\")\n",
        "  initial_model_path = initial_model_log_df[initial_model_log_df[\"model_name\"] == initial_model_name][\"path\"].values[0]\n",
        "  initial_model_round_num = initial_model_log_df[initial_model_log_df[\"model_name\"] == initial_model_name][\"round_number\"].values[0]\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  dino_model = torch.hub.load('facebookresearch/dino:main', backbone)\n",
        "  initial_model = DinoClassifier(dino_model=dino_model, num_classes=100, device=device)\n",
        "  initial_model.load_state_dict(torch.load(initial_model_path))\n",
        "\n",
        "  client_data = torch.load(path_to_subsets, weights_only=False)\n",
        "  if not spliting_method == \"i.i.d. sharing\":\n",
        "    class_combs = torch.load(path_to_class_combs)\n",
        "\n",
        "\n",
        "method = \"iid\" if spliting_method == \"i.i.d. sharing\" else \"noniid\"\n"
      ],
      "metadata": {
        "id": "56FLr8HaszZz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #@title Extraction of the model we want to use as initial model\n",
        "# if load_data:\n",
        "#   global_log = pd.read_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\")\n",
        "\n",
        "\n",
        "#   filter = (global_log[\"aggregation_method\"] == \"EMA\") & (global_log[\"round_number\"]==2)\n",
        "#   filtered_models = global_log[filter]\n",
        "\n",
        "#   print(filtered_models[\"model_name\"].values)\n",
        "\n",
        "#   filtered_models.head()\n"
      ],
      "metadata": {
        "id": "x1fIsj0tMGjo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "save_data = False #@param{\"type\":\"boolean\"}\n",
        "\n",
        "try :\n",
        "  if load_data:\n",
        "    save_data = False\n",
        "except:\n",
        "  pass\n",
        "if save_data:\n",
        "  method = \"iid\" if spliting_method == \"i.i.d. sharing\" else \"noniid\"\n",
        "  path_to_subsets = f\"/content/drive/MyDrive/MLDL_FederatedLearning/client_subsets/client_data_{method}_{str(n_clients)}clients_{str(uuid4())}.pth\"\n",
        "  if spliting_method == \"i.i.d. sharing\":\n",
        "    client_data = iid_sharing(train_set, n_clients)\n",
        "    class_combs = \"all\"\n",
        "    print(spliting_method)\n",
        "  else:\n",
        "    client_data, class_combs = noniid_sharing(train_set,Nc=Nc, n_clients=n_clients)\n",
        "    path_to_class_combs = \"/content/drive/MyDrive/MLDL_FederatedLearning/client_subsets/class_combs_\"+method + str(n_clients)+\"clients_\"+str(uuid4())+\".pth\"\n",
        "    print(spliting_method,path_to_class_combs)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  dino_model = torch.hub.load('facebookresearch/dino:main', backbone)\n",
        "  initial_model = DinoClassifier(dino_model=dino_model, num_classes=100, device=device)\n",
        "  initial_model_name = next_id(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\")\n",
        "  initial_model_path = \"/content/drive/MyDrive/MLDL_FederatedLearning/models/global/\" + initial_model_name + \".pth\"\n",
        "  initial_model_round_num = 0\n",
        "  initial_model_log = {\n",
        "    \"backbone\": [backbone],\n",
        "    \"model_name\": [initial_model_name],\n",
        "    \"num_of_clients\": [n_clients],\n",
        "    \"path\": [initial_model_path],\n",
        "    \"Measurement_criteria\": [None],\n",
        "    \"prev_global_model_name\":[None],\n",
        "    \"accuracy\": [None],\n",
        "    \"loss\": [None],\n",
        "    \"splitting_method\": [spliting_method],\n",
        "    \"size_of_dataset\": [len(train_dataset)],\n",
        "    \"train_test_ratio\": [None],\n",
        "    \"classes\": [None],\n",
        "    \"round_number\": [0],\n",
        "    \"time\": [get_current_time()],\n",
        "    \"path_to_subsets\": [path_to_subsets],\n",
        "    \"path_to_class_combs\": [path_to_class_combs],\n",
        "    \"num_of_participants\": [None]\n",
        "}\n",
        "  initial_model_log[\"aggregation_method\"] =[ np.nan]\n",
        "  initial_model_log[\"contributors\"] =[ np.nan]\n",
        "  initial_model_log[\"momentum_vector_path\"] = [np.nan]\n",
        "\n",
        "\n",
        "  initial_model_log = pd.DataFrame(initial_model_log)\n",
        "  initial_model_log = initial_model_log[['backbone',\n",
        "                'num_of_clients',\n",
        "                'splitting_method',\n",
        "                'aggregation_method',\n",
        "                'Measurement_criteria',\n",
        "                'accuracy',\n",
        "                'loss',\n",
        "                'size_of_dataset',\n",
        "                'train_test_ratio',\n",
        "                'classes',\n",
        "                'round_number',\n",
        "                'num_of_participants',\n",
        "                'model_name',\n",
        "                'prev_global_model_name',\n",
        "                \"contributors\",\n",
        "                'path',\n",
        "                \"momentum_vector_path\",\n",
        "                'path_to_subsets',\n",
        "                'path_to_class_combs',\n",
        "                'time'\n",
        "                ]]\n",
        "  if not os.path.exists(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\"):\n",
        "    initial_model_log.to_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\", index=False)\n",
        "  else:\n",
        "    initial_model_log.to_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\", mode='a', header=False, index=False)\n",
        "\n",
        "  torch.save(initial_model.state_dict(), initial_model_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  torch.save(client_data, path_to_subsets)\n",
        "  if method== \"noniid\":\n",
        "    torch.save(class_combs, path_to_class_combs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FSnlxpSszYyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f6ece8-22ea-45fe-9971-2e299cf6076d",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "non-i.i.d. sharing /content/drive/MyDrive/MLDL_FederatedLearning/client_subsets/class_combs_noniid80clients_cefcc7c4-0345-4722-8b49-35935aa5a6a0.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n",
            "100%|██████████| 82.7M/82.7M [00:00<00:00, 175MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected feature dimontion: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('path to subsets:', path_to_subsets)\n",
        "if method== \"noniid\":\n",
        "  print('path to class combs:', path_to_class_combs)\n",
        "print('initial model name:', initial_model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4edceTzcG1pW",
        "outputId": "febabfaa-9d9b-483e-81a6-dbf03adb71bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path to subsets: /content/drive/MyDrive/MLDL_FederatedLearning/client_subsets/client_data_noniid_80clients_42bf9d0a-c322-4e3d-b9ef-e19017a47025.pth\n",
            "path to class combs: /content/drive/MyDrive/MLDL_FederatedLearning/client_subsets/class_combs_noniid80clients_cefcc7c4-0345-4722-8b49-35935aa5a6a0.pth\n",
            "initial model name: b8437215-ee1e-48a4-8a5c-1fbfabaf6dcb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Random clients selection\n",
        "\n",
        "global_log = pd.read_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/global_log.csv\")\n",
        "\n",
        "\n",
        "filter = (global_log[\"model_name\"] == initial_model_name)\n",
        "filtered_models = global_log[filter]\n",
        "\n",
        "r_num = filtered_models[\"round_number\"].values[0]\n",
        "\n",
        "selection_percentage = 15 #@param {\"type\":\"integer\"}\n",
        "set_seed(int(r_num),is_seed_fixed)\n",
        "def get_random_clients(n_clients, initial_model_name, selection_percentage=10):\n",
        "  if os.path.exists(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\"):\n",
        "    clients_df = pd.read_csv(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\")\n",
        "    clients_df = clients_df[clients_df['initial_model_name']== initial_model_name]\n",
        "    selected_clients = clients_df['client_id'].values\n",
        "  else:\n",
        "    selected_clients = np.array([], dtype=np.int16)\n",
        "  while len(selected_clients) < (selection_percentage / 100 ) * n_clients:\n",
        "    rand_int = torch.randint(0,n_clients,(1,))[0].item()\n",
        "    if rand_int not in selected_clients:\n",
        "      selected_clients = np.append(selected_clients,rand_int)\n",
        "\n",
        "  return selected_clients\n",
        "\n",
        "selected_clients = get_random_clients(n_clients,initial_model_name,selection_percentage)\n",
        "print(selected_clients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5y_rFmsGx22",
        "outputId": "bd049184-de4c-4f53-96a2-c3516fbb3b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[44 79 53  0  3 59 67 23 57 21 66 36]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_model_editing_active = True #@param{\"type\":\"boolean\"}"
      ],
      "metadata": {
        "id": "mFQjRhUv7Oki",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmcRGW2lVdE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e457db0-4e7f-4aaf-a5c3-7e9397555405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 / 12 ####################################################################################################\n",
            "Data size:  463\n",
            "Backbone:  dino_vits16\n",
            "client 44, Training- epoch 0 - epoch loss:5.0702\n",
            "client 44, Training- epoch 1 - epoch loss:4.8853\n",
            "client 44, Training- epoch 2 - epoch loss:4.6751\n",
            "client 44, Training- epoch 3 - epoch loss:4.5089\n",
            "client 44, Training- epoch 4 - epoch loss:4.3674\n",
            "client 44, Training- epoch 5 - epoch loss:4.2591\n",
            "client 44, Training- epoch 6 - epoch loss:4.1721\n",
            "client 44, Training- epoch 7 - epoch loss:4.0685\n",
            "client 44, Training- epoch 8 - epoch loss:3.9491\n",
            "client 44, Training- epoch 9 - epoch loss:3.8607\n",
            "Client 44- accuracy: 20.0, loss: 3.8136876235137116\n",
            "Trained but week classes: [92, 29, 76, 60, 13, 1, 86, 35, 89, 74, 17, 65, 66, 26, 71, 52, 24, 64, 94, 40, 8, 85, 37, 90, 95]\n",
            "Client 44 - 98 Weak classes detected, Number of included classes: 25, Trained but week classes: 25\n",
            "client 44, Model editing- epoch 0 - epoch loss:4.4593\n",
            "client 44, Model editing- epoch 1 - epoch loss:4.3274\n",
            "client 44, Model editing- epoch 2 - epoch loss:4.1564\n",
            "client 44, Model editing- epoch 3 - epoch loss:4.0146\n",
            "client 44, Model editing- epoch 4 - epoch loss:3.9235\n",
            "client 44, Model editing- epoch 5 - epoch loss:3.8232\n",
            "client 44, Model editing- epoch 6 - epoch loss:3.7556\n",
            "client 44, Model editing- epoch 7 - epoch loss:3.6484\n",
            "client 44, Model editing- epoch 8 - epoch loss:3.6057\n",
            "client 44, Model editing- epoch 9 - epoch loss:3.4902\n",
            "Client 44- accuracy: 18.64864864864865, loss: 3.7668813718331826\n",
            "name: f41bfa3d-a35b-4f63-a857-4d21e07b354d \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/25ec2745-7bb8-4486-900c-a4983dc8ff64.pth \n",
            "Logged client 44 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "2 / 12 ####################################################################################################\n",
            "Data size:  277\n",
            "Backbone:  dino_vits16\n",
            "client 79, Training- epoch 0 - epoch loss:5.2023\n",
            "client 79, Training- epoch 1 - epoch loss:5.0752\n",
            "client 79, Training- epoch 2 - epoch loss:4.8812\n",
            "client 79, Training- epoch 3 - epoch loss:4.6640\n",
            "client 79, Training- epoch 4 - epoch loss:4.5085\n",
            "client 79, Training- epoch 5 - epoch loss:4.3965\n",
            "client 79, Training- epoch 6 - epoch loss:4.2673\n",
            "client 79, Training- epoch 7 - epoch loss:4.1422\n",
            "client 79, Training- epoch 8 - epoch loss:4.0544\n",
            "client 79, Training- epoch 9 - epoch loss:4.0070\n",
            "Client 79- accuracy: 9.95475113122172, loss: 3.931461447504311\n",
            "Trained but week classes: [7, 79, 33, 60, 22, 77, 58, 53, 93, 55, 19, 81, 12, 51, 26, 90, 47, 37, 1, 4, 9]\n",
            "Client 79 - 88 Weak classes detected, Number of included classes: 25, Trained but week classes: 21\n",
            "client 79, Model editing- epoch 0 - epoch loss:4.6056\n",
            "client 79, Model editing- epoch 1 - epoch loss:4.5178\n",
            "client 79, Model editing- epoch 2 - epoch loss:4.3864\n",
            "client 79, Model editing- epoch 3 - epoch loss:4.2436\n",
            "client 79, Model editing- epoch 4 - epoch loss:4.1227\n",
            "client 79, Model editing- epoch 5 - epoch loss:4.0281\n",
            "client 79, Model editing- epoch 6 - epoch loss:3.9107\n",
            "client 79, Model editing- epoch 7 - epoch loss:3.8178\n",
            "client 79, Model editing- epoch 8 - epoch loss:3.7474\n",
            "client 79, Model editing- epoch 9 - epoch loss:3.6649\n",
            "Client 79- accuracy: 12.217194570135746, loss: 3.9496811266937946\n",
            "name: c3d4f3a4-004f-4029-b83b-e45c6904d445 \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/a2573ab5-d227-424b-9afd-3856f13826e7.pth \n",
            "Logged client 79 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "3 / 12 ####################################################################################################\n",
            "Data size:  483\n",
            "Backbone:  dino_vits16\n",
            "client 53, Training- epoch 0 - epoch loss:5.0067\n",
            "client 53, Training- epoch 1 - epoch loss:4.8152\n",
            "client 53, Training- epoch 2 - epoch loss:4.6800\n",
            "client 53, Training- epoch 3 - epoch loss:4.5039\n",
            "client 53, Training- epoch 4 - epoch loss:4.4311\n",
            "client 53, Training- epoch 5 - epoch loss:4.2561\n",
            "client 53, Training- epoch 6 - epoch loss:4.1927\n",
            "client 53, Training- epoch 7 - epoch loss:4.1577\n",
            "client 53, Training- epoch 8 - epoch loss:4.0535\n",
            "client 53, Training- epoch 9 - epoch loss:3.9079\n",
            "Client 53- accuracy: 18.911917098445596, loss: 3.858293013251507\n",
            "Trained but week classes: [13, 68, 98, 66, 95, 37, 52, 97, 73, 65, 69, 17, 36, 24, 88, 21, 26, 82, 5, 38, 12, 25, 81, 80, 77]\n",
            "Client 53 - 97 Weak classes detected, Number of included classes: 25, Trained but week classes: 25\n",
            "client 53, Model editing- epoch 0 - epoch loss:4.0949\n",
            "client 53, Model editing- epoch 1 - epoch loss:3.9455\n",
            "client 53, Model editing- epoch 2 - epoch loss:3.8362\n",
            "client 53, Model editing- epoch 3 - epoch loss:3.7118\n",
            "client 53, Model editing- epoch 4 - epoch loss:3.6155\n",
            "client 53, Model editing- epoch 5 - epoch loss:3.5313\n",
            "client 53, Model editing- epoch 6 - epoch loss:3.4420\n",
            "client 53, Model editing- epoch 7 - epoch loss:3.3734\n",
            "client 53, Model editing- epoch 8 - epoch loss:3.2927\n",
            "client 53, Model editing- epoch 9 - epoch loss:3.2318\n",
            "Client 53- accuracy: 19.430051813471504, loss: 3.763311423168281\n",
            "name: 0f88f0b4-3523-4c5a-85eb-001d63167505 \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/bc64ee5c-496c-4b4d-91b4-1dc6f48674d2.pth \n",
            "Logged client 53 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "4 / 12 ####################################################################################################\n",
            "Data size:  533\n",
            "Backbone:  dino_vits16\n",
            "client 0, Training- epoch 0 - epoch loss:5.0239\n",
            "client 0, Training- epoch 1 - epoch loss:4.8190\n",
            "client 0, Training- epoch 2 - epoch loss:4.6463\n",
            "client 0, Training- epoch 3 - epoch loss:4.4927\n",
            "client 0, Training- epoch 4 - epoch loss:4.4045\n",
            "client 0, Training- epoch 5 - epoch loss:4.2891\n",
            "client 0, Training- epoch 6 - epoch loss:4.1948\n",
            "client 0, Training- epoch 7 - epoch loss:4.0847\n",
            "client 0, Training- epoch 8 - epoch loss:4.0038\n",
            "client 0, Training- epoch 9 - epoch loss:3.9026\n",
            "Client 0- accuracy: 18.779342723004696, loss: 3.857226230728794\n",
            "Trained but week classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24]\n",
            "Client 0 - 98 Weak classes detected, Number of included classes: 25, Trained but week classes: 24\n",
            "client 0, Model editing- epoch 0 - epoch loss:4.4412\n",
            "client 0, Model editing- epoch 1 - epoch loss:4.2718\n",
            "client 0, Model editing- epoch 2 - epoch loss:4.1413\n",
            "client 0, Model editing- epoch 3 - epoch loss:3.9514\n",
            "client 0, Model editing- epoch 4 - epoch loss:3.8870\n",
            "client 0, Model editing- epoch 5 - epoch loss:3.7955\n",
            "client 0, Model editing- epoch 6 - epoch loss:3.7016\n",
            "client 0, Model editing- epoch 7 - epoch loss:3.6411\n",
            "client 0, Model editing- epoch 8 - epoch loss:3.5603\n",
            "client 0, Model editing- epoch 9 - epoch loss:3.4680\n",
            "Client 0- accuracy: 20.422535211267604, loss: 3.768154372631664\n",
            "name: 9dccc91c-84ce-48d1-af80-f620b1d34400 \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/53df59e2-4d80-4b85-acb8-40fbf2a34a92.pth \n",
            "Logged client 0 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "5 / 12 ####################################################################################################\n",
            "Data size:  516\n",
            "Backbone:  dino_vits16\n",
            "client 3, Training- epoch 0 - epoch loss:5.1439\n",
            "client 3, Training- epoch 1 - epoch loss:4.9361\n",
            "client 3, Training- epoch 2 - epoch loss:4.7439\n",
            "client 3, Training- epoch 3 - epoch loss:4.5699\n",
            "client 3, Training- epoch 4 - epoch loss:4.4326\n",
            "client 3, Training- epoch 5 - epoch loss:4.3064\n",
            "client 3, Training- epoch 6 - epoch loss:4.2164\n",
            "client 3, Training- epoch 7 - epoch loss:4.1119\n",
            "client 3, Training- epoch 8 - epoch loss:4.0312\n",
            "client 3, Training- epoch 9 - epoch loss:3.9310\n",
            "Client 3- accuracy: 16.019417475728154, loss: 3.8582825938474783\n",
            "Trained but week classes: [75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Client 3 - 98 Weak classes detected, Number of included classes: 25, Trained but week classes: 25\n",
            "client 3, Model editing- epoch 0 - epoch loss:4.4441\n",
            "client 3, Model editing- epoch 1 - epoch loss:4.2815\n",
            "client 3, Model editing- epoch 2 - epoch loss:4.1436\n",
            "client 3, Model editing- epoch 3 - epoch loss:4.0245\n",
            "client 3, Model editing- epoch 4 - epoch loss:3.8971\n",
            "client 3, Model editing- epoch 5 - epoch loss:3.8234\n",
            "client 3, Model editing- epoch 6 - epoch loss:3.7291\n",
            "client 3, Model editing- epoch 7 - epoch loss:3.6667\n",
            "client 3, Model editing- epoch 8 - epoch loss:3.5623\n",
            "client 3, Model editing- epoch 9 - epoch loss:3.4893\n",
            "Client 3- accuracy: 18.203883495145632, loss: 3.7630717499742232\n",
            "name: 9fb8db17-5814-46b7-b613-73705658e9bc \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/9cac9052-d1fa-4639-8105-7e2e0c631672.pth \n",
            "Logged client 3 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "6 / 12 ####################################################################################################\n",
            "Data size:  516\n",
            "Backbone:  dino_vits16\n",
            "client 59, Training- epoch 0 - epoch loss:5.0423\n",
            "client 59, Training- epoch 1 - epoch loss:4.8509\n",
            "client 59, Training- epoch 2 - epoch loss:4.6512\n",
            "client 59, Training- epoch 3 - epoch loss:4.4960\n",
            "client 59, Training- epoch 4 - epoch loss:4.3630\n",
            "client 59, Training- epoch 5 - epoch loss:4.2598\n",
            "client 59, Training- epoch 6 - epoch loss:4.1438\n",
            "client 59, Training- epoch 7 - epoch loss:4.0564\n",
            "client 59, Training- epoch 8 - epoch loss:3.9714\n",
            "client 59, Training- epoch 9 - epoch loss:3.8916\n",
            "Client 59- accuracy: 17.475728155339805, loss: 3.82834742370161\n",
            "Trained but week classes: [31, 70, 66, 85, 94, 98, 43, 93, 12, 34, 76, 72, 21, 7, 2, 79, 87, 83, 53, 84, 55, 52, 69, 64, 86]\n",
            "Client 59 - 96 Weak classes detected, Number of included classes: 25, Trained but week classes: 25\n",
            "client 59, Model editing- epoch 0 - epoch loss:4.4540\n",
            "client 59, Model editing- epoch 1 - epoch loss:4.3077\n",
            "client 59, Model editing- epoch 2 - epoch loss:4.1648\n",
            "client 59, Model editing- epoch 3 - epoch loss:4.0335\n",
            "client 59, Model editing- epoch 4 - epoch loss:3.9281\n",
            "client 59, Model editing- epoch 5 - epoch loss:3.8463\n",
            "client 59, Model editing- epoch 6 - epoch loss:3.7244\n",
            "client 59, Model editing- epoch 7 - epoch loss:3.6643\n",
            "client 59, Model editing- epoch 8 - epoch loss:3.5900\n",
            "client 59, Model editing- epoch 9 - epoch loss:3.5014\n",
            "Client 59- accuracy: 17.233009708737864, loss: 3.775141729891879\n",
            "name: 8902fa34-2442-4b82-b70c-92d245bdf370 \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/4f09cd5d-da6a-4d86-b1f8-0606fbee8eaf.pth \n",
            "Logged client 59 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "7 / 12 ####################################################################################################\n",
            "Data size:  513\n",
            "Backbone:  dino_vits16\n",
            "client 67, Training- epoch 0 - epoch loss:5.0635\n",
            "client 67, Training- epoch 1 - epoch loss:4.8455\n",
            "client 67, Training- epoch 2 - epoch loss:4.6452\n",
            "client 67, Training- epoch 3 - epoch loss:4.4772\n",
            "client 67, Training- epoch 4 - epoch loss:4.3334\n",
            "client 67, Training- epoch 5 - epoch loss:4.2425\n",
            "client 67, Training- epoch 6 - epoch loss:4.1612\n",
            "client 67, Training- epoch 7 - epoch loss:4.0453\n",
            "client 67, Training- epoch 8 - epoch loss:3.9667\n",
            "client 67, Training- epoch 9 - epoch loss:3.8708\n",
            "Client 67- accuracy: 18.78048780487805, loss: 3.8052036529634057\n",
            "Trained but week classes: [36, 62, 31, 25, 46, 18, 99, 17, 58, 95, 9, 1, 3, 30, 42, 67, 51, 71, 85, 29, 8, 20, 43, 68, 90]\n",
            "Client 67 - 100 Weak classes detected, Number of included classes: 25, Trained but week classes: 25\n",
            "client 67, Model editing- epoch 0 - epoch loss:4.3959\n",
            "client 67, Model editing- epoch 1 - epoch loss:4.2702\n",
            "client 67, Model editing- epoch 2 - epoch loss:4.1411\n",
            "client 67, Model editing- epoch 3 - epoch loss:4.0153\n",
            "client 67, Model editing- epoch 4 - epoch loss:3.8952\n",
            "client 67, Model editing- epoch 5 - epoch loss:3.7987\n",
            "client 67, Model editing- epoch 6 - epoch loss:3.7079\n",
            "client 67, Model editing- epoch 7 - epoch loss:3.6477\n",
            "client 67, Model editing- epoch 8 - epoch loss:3.5544\n",
            "client 67, Model editing- epoch 9 - epoch loss:3.5049\n",
            "Client 67- accuracy: 18.536585365853657, loss: 3.6880164797713118\n",
            "name: 5a15ffe5-6880-487a-9859-4dd44e36cc02 \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/294c94f8-89d2-4f84-a9be-092853d5ec3c.pth \n",
            "Logged client 67 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "8 / 12 ####################################################################################################\n",
            "Data size:  507\n",
            "Backbone:  dino_vits16\n",
            "client 23, Training- epoch 0 - epoch loss:5.2001\n",
            "client 23, Training- epoch 1 - epoch loss:4.9563\n",
            "client 23, Training- epoch 2 - epoch loss:4.7714\n",
            "client 23, Training- epoch 3 - epoch loss:4.5989\n",
            "client 23, Training- epoch 4 - epoch loss:4.4592\n",
            "client 23, Training- epoch 5 - epoch loss:4.3617\n",
            "client 23, Training- epoch 6 - epoch loss:4.2606\n",
            "client 23, Training- epoch 7 - epoch loss:4.1575\n",
            "client 23, Training- epoch 8 - epoch loss:4.0726\n",
            "client 23, Training- epoch 9 - epoch loss:4.0005\n",
            "Client 23- accuracy: 15.061728395061728, loss: 3.919748388690713\n",
            "Trained but week classes: [29, 86, 34, 75, 41, 8, 13, 10, 24, 6, 21, 12, 46, 69, 36, 25, 1, 33, 63, 68, 48, 9, 94, 71, 81]\n",
            "Client 23 - 98 Weak classes detected, Number of included classes: 25, Trained but week classes: 25\n",
            "client 23, Model editing- epoch 0 - epoch loss:4.4036\n",
            "client 23, Model editing- epoch 1 - epoch loss:4.2705\n",
            "client 23, Model editing- epoch 2 - epoch loss:4.1383\n",
            "client 23, Model editing- epoch 3 - epoch loss:4.0168\n",
            "client 23, Model editing- epoch 4 - epoch loss:3.9011\n",
            "client 23, Model editing- epoch 5 - epoch loss:3.8105\n",
            "client 23, Model editing- epoch 6 - epoch loss:3.7253\n",
            "client 23, Model editing- epoch 7 - epoch loss:3.6418\n",
            "client 23, Model editing- epoch 8 - epoch loss:3.5563\n",
            "client 23, Model editing- epoch 9 - epoch loss:3.4801\n",
            "Client 23- accuracy: 19.012345679012345, loss: 3.8211456334149396\n",
            "name: a7bd6428-cff1-42c9-a512-59d4ba717192 \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/bee26ee0-48fc-4d6b-b8d2-8e6257254b6c.pth \n",
            "Logged client 23 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "9 / 12 ####################################################################################################\n",
            "Data size:  484\n",
            "Backbone:  dino_vits16\n",
            "client 57, Training- epoch 0 - epoch loss:5.0944\n",
            "client 57, Training- epoch 1 - epoch loss:4.9450\n",
            "client 57, Training- epoch 2 - epoch loss:4.6650\n",
            "client 57, Training- epoch 3 - epoch loss:4.5964\n",
            "client 57, Training- epoch 4 - epoch loss:4.4150\n",
            "client 57, Training- epoch 5 - epoch loss:4.3304\n",
            "client 57, Training- epoch 6 - epoch loss:4.1954\n",
            "client 57, Training- epoch 7 - epoch loss:4.1342\n",
            "client 57, Training- epoch 8 - epoch loss:4.0089\n",
            "client 57, Training- epoch 9 - epoch loss:3.9445\n",
            "Client 57- accuracy: 20.671834625322997, loss: 3.863441275687797\n",
            "Trained but week classes: [29, 68, 19, 90, 22, 70, 15, 4, 10, 14, 81, 63, 88, 16, 8, 50, 46, 11, 56, 12, 80, 35, 40]\n",
            "Client 57 - 96 Weak classes detected, Number of included classes: 25, Trained but week classes: 23\n",
            "client 57, Model editing- epoch 0 - epoch loss:4.1074\n",
            "client 57, Model editing- epoch 1 - epoch loss:3.9711\n",
            "client 57, Model editing- epoch 2 - epoch loss:3.8242\n",
            "client 57, Model editing- epoch 3 - epoch loss:3.7120\n",
            "client 57, Model editing- epoch 4 - epoch loss:3.6073\n",
            "client 57, Model editing- epoch 5 - epoch loss:3.5161\n",
            "client 57, Model editing- epoch 6 - epoch loss:3.4384\n",
            "client 57, Model editing- epoch 7 - epoch loss:3.3679\n",
            "client 57, Model editing- epoch 8 - epoch loss:3.2944\n",
            "client 57, Model editing- epoch 9 - epoch loss:3.2147\n",
            "Client 57- accuracy: 16.795865633074936, loss: 3.8136983681710808\n",
            "name: 49981b50-4aba-4e13-8ccf-20f7a99c2ec3 \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/2bde1bb3-6f7e-4df3-9953-65cd45dee41c.pth \n",
            "Logged client 57 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "10 / 12 ####################################################################################################\n",
            "Data size:  502\n",
            "Backbone:  dino_vits16\n",
            "client 21, Training- epoch 0 - epoch loss:5.0856\n",
            "client 21, Training- epoch 1 - epoch loss:4.8823\n",
            "client 21, Training- epoch 2 - epoch loss:4.6918\n",
            "client 21, Training- epoch 3 - epoch loss:4.5309\n",
            "client 21, Training- epoch 4 - epoch loss:4.3872\n",
            "client 21, Training- epoch 5 - epoch loss:4.2976\n",
            "client 21, Training- epoch 6 - epoch loss:4.1645\n",
            "client 21, Training- epoch 7 - epoch loss:4.0917\n",
            "client 21, Training- epoch 8 - epoch loss:3.9901\n",
            "client 21, Training- epoch 9 - epoch loss:3.8927\n",
            "Client 21- accuracy: 16.458852867830423, loss: 3.8418582306241156\n",
            "Trained but week classes: [28, 55, 64, 42, 76, 97, 53, 37, 93, 26, 62, 30, 27, 31, 9, 91, 36, 7, 82, 22, 0, 16, 34, 69]\n",
            "Client 21 - 98 Weak classes detected, Number of included classes: 25, Trained but week classes: 24\n",
            "client 21, Model editing- epoch 0 - epoch loss:4.5118\n",
            "client 21, Model editing- epoch 1 - epoch loss:4.3267\n",
            "client 21, Model editing- epoch 2 - epoch loss:4.1577\n",
            "client 21, Model editing- epoch 3 - epoch loss:4.0028\n",
            "client 21, Model editing- epoch 4 - epoch loss:3.8974\n",
            "client 21, Model editing- epoch 5 - epoch loss:3.8010\n",
            "client 21, Model editing- epoch 6 - epoch loss:3.7129\n",
            "client 21, Model editing- epoch 7 - epoch loss:3.6184\n",
            "client 21, Model editing- epoch 8 - epoch loss:3.5258\n",
            "client 21, Model editing- epoch 9 - epoch loss:3.4629\n",
            "Client 21- accuracy: 18.952618453865338, loss: 3.7815242931432556\n",
            "name: fff1217a-e7af-42ad-ace5-acf92169b5f4 \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/e089e346-978f-4aa7-950c-2cdd9a3f0474.pth \n",
            "Logged client 21 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "11 / 12 ####################################################################################################\n",
            "Data size:  511\n",
            "Backbone:  dino_vits16\n",
            "client 66, Training- epoch 0 - epoch loss:5.0865\n",
            "client 66, Training- epoch 1 - epoch loss:4.8818\n",
            "client 66, Training- epoch 2 - epoch loss:4.6986\n",
            "client 66, Training- epoch 3 - epoch loss:4.5081\n",
            "client 66, Training- epoch 4 - epoch loss:4.3981\n",
            "client 66, Training- epoch 5 - epoch loss:4.2863\n",
            "client 66, Training- epoch 6 - epoch loss:4.1752\n",
            "client 66, Training- epoch 7 - epoch loss:4.0951\n",
            "client 66, Training- epoch 8 - epoch loss:4.0292\n",
            "client 66, Training- epoch 9 - epoch loss:3.9401\n",
            "Client 66- accuracy: 19.11764705882353, loss: 3.869394110698326\n",
            "Trained but week classes: [93, 17, 60, 40, 83, 70, 92, 42, 19, 75, 51, 56, 46, 76, 44, 61, 8, 79, 1, 82, 50, 55, 9, 10]\n",
            "Client 66 - 98 Weak classes detected, Number of included classes: 25, Trained but week classes: 24\n",
            "client 66, Model editing- epoch 0 - epoch loss:4.4293\n",
            "client 66, Model editing- epoch 1 - epoch loss:4.2851\n",
            "client 66, Model editing- epoch 2 - epoch loss:4.1340\n",
            "client 66, Model editing- epoch 3 - epoch loss:4.0213\n",
            "client 66, Model editing- epoch 4 - epoch loss:3.9213\n",
            "client 66, Model editing- epoch 5 - epoch loss:3.8286\n",
            "client 66, Model editing- epoch 6 - epoch loss:3.7290\n",
            "client 66, Model editing- epoch 7 - epoch loss:3.6549\n",
            "client 66, Model editing- epoch 8 - epoch loss:3.5617\n",
            "client 66, Model editing- epoch 9 - epoch loss:3.4860\n",
            "Client 66- accuracy: 18.38235294117647, loss: 3.781882094401939\n",
            "name: 5dd96d3a-7689-429b-88c3-4be915bd06cd \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/7b8ed5a8-de63-4879-977e-7cd6d12b19b0.pth \n",
            "Logged client 66 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n",
            "12 / 12 ####################################################################################################\n",
            "Data size:  527\n",
            "Backbone:  dino_vits16\n",
            "client 36, Training- epoch 0 - epoch loss:5.1092\n",
            "client 36, Training- epoch 1 - epoch loss:4.9553\n",
            "client 36, Training- epoch 2 - epoch loss:4.6905\n",
            "client 36, Training- epoch 3 - epoch loss:4.4855\n",
            "client 36, Training- epoch 4 - epoch loss:4.4462\n",
            "client 36, Training- epoch 5 - epoch loss:4.2792\n",
            "client 36, Training- epoch 6 - epoch loss:4.1890\n",
            "client 36, Training- epoch 7 - epoch loss:4.1534\n",
            "client 36, Training- epoch 8 - epoch loss:4.0009\n",
            "client 36, Training- epoch 9 - epoch loss:3.9239\n",
            "Client 36- accuracy: 17.577197149643705, loss: 3.867589495244332\n",
            "Trained but week classes: [97, 59, 30, 22, 88, 79, 90, 53, 81, 32, 18, 26, 28, 61, 39, 43, 5, 80, 15, 45, 71, 8, 23, 86]\n",
            "Client 36 - 99 Weak classes detected, Number of included classes: 25, Trained but week classes: 24\n",
            "client 36, Model editing- epoch 0 - epoch loss:4.5300\n",
            "client 36, Model editing- epoch 1 - epoch loss:4.2782\n",
            "client 36, Model editing- epoch 2 - epoch loss:4.1951\n",
            "client 36, Model editing- epoch 3 - epoch loss:4.0642\n",
            "client 36, Model editing- epoch 4 - epoch loss:3.8899\n",
            "client 36, Model editing- epoch 5 - epoch loss:3.8087\n",
            "client 36, Model editing- epoch 6 - epoch loss:3.6700\n",
            "client 36, Model editing- epoch 7 - epoch loss:3.6158\n",
            "client 36, Model editing- epoch 8 - epoch loss:3.5722\n",
            "client 36, Model editing- epoch 9 - epoch loss:3.5095\n",
            "Client 36- accuracy: 17.577197149643705, loss: 3.8778857765741415\n",
            "name: 5d33ee02-2a9b-473c-aa5e-b560a04af9e9 \n",
            "path: /content/drive/MyDrive/MLDL_FederatedLearning/models/clients/79f793fd-6e81-4638-807c-a77c744169e5.pth \n",
            "Logged client 36 to /content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\n"
          ]
        }
      ],
      "source": [
        "set_seed(seed,is_seed_fixed)\n",
        "\n",
        "\n",
        "if spliting_method == \"i.i.d. sharing\":\n",
        "  path_to_class_combs = \" \"\n",
        "\n",
        "\n",
        "log_file = \"/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\"\n",
        "\n",
        "counter = 1\n",
        "for client_num in selected_clients:\n",
        "      print(counter,\"/\",len(selected_clients), \"#\"*100)\n",
        "      counter+=1\n",
        "\n",
        "      if os.path.exists(\"/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv\"):\n",
        "        all_clients_df = pd.read_csv(log_file)\n",
        "        filtered_clients_df = all_clients_df[all_clients_df[\"initial_model_name\"] == initial_model_name]\n",
        "        if client_num in filtered_clients_df[\"client_id\"].values:\n",
        "          print(f\"Client {client_num} is already trained\")\n",
        "          continue\n",
        "    # try:\n",
        "      client = Client(id=client_num,\n",
        "                    data=client_data[client_num] ,\n",
        "                    spliting_method=spliting_method,\n",
        "                    classes=\"all\",\n",
        "                    n_clients=n_clients,\n",
        "                    batch_size = batch_size,\n",
        "                    num_epochs= 10,\n",
        "                    grad_mask= True,\n",
        "                    initial_model = copy.deepcopy(initial_model),\n",
        "                    backbone=backbone,\n",
        "                    path_to_model=None,\n",
        "                    spliting_ratio={\"train\":0.8, \"test\":0.2},\n",
        "                    path_to_subsets=path_to_subsets,\n",
        "                    path_to_class_combs=path_to_class_combs\n",
        "                    )\n",
        "      print(\"Data size: \",len(client_data[client_num]))\n",
        "      print(\"Backbone: \", backbone)\n",
        "\n",
        "\n",
        "\n",
        "      client.train()\n",
        "      client.evaluate()\n",
        "      print(f\"Client {client_num}- accuracy: {client.accuracy}, loss: {client.loss}\", )\n",
        "\n",
        "      if is_model_editing_active:\n",
        "      ###PROVA\n",
        "      # Call linear_probe_eval from the model instance, passing necessary arguments\n",
        "        class_accuracy, weak_classes = client.model.linear_probe_eval(client.train_set, client.device,accuracy_threshold=50)\n",
        "\n",
        "        if weak_classes:\n",
        "          comment_classes= []\n",
        "          for  __cls in class_combs[client_num]:\n",
        "            if __cls.item() in weak_classes:\n",
        "              comment_classes.append(__cls.item())\n",
        "          print(\"Trained but week classes:\",comment_classes)\n",
        "          print(f\"Client {client_num} - {len(weak_classes)} Weak classes detected, Number of included classes: {len(class_combs[client_num])}, Trained but week classes: {len(comment_classes)}\") # Added client id print here\n",
        "\n",
        "\n",
        "          client.model_edit(weak_classes,num_epochs=10) # Call model_edit from the model instance\n",
        "          client.evaluate()\n",
        "          print(f\"Client {client_num}- accuracy: {client.accuracy}, loss: {client.loss}\", )\n",
        "\n",
        "      ##FINEPROVA\n",
        "\n",
        "      # Use the save_client method from the Client class\n",
        "      log = client.create_log(\n",
        "          model_name=next_id(log_file), # Generate a new model name\n",
        "          path=f\"/content/drive/MyDrive/MLDL_FederatedLearning/models/clients/{next_id(log_file)}.pth\", # Generate a new path\n",
        "          round_number=initial_model_round_num + 1\n",
        "          )\n",
        "      client.confirm_save(log['path'][0]) # Save the model\n",
        "\n",
        "      if not os.path.exists(log_file):\n",
        "        log.to_csv(log_file, index=False)\n",
        "\n",
        "        print(\"new csv file \")\n",
        "        print(f\"name: {log['model_name'][0]} \")\n",
        "        print(f\"path: {log['path'][0]} \")\n",
        "        print(f\"Logged client {client_num} to {log_file}\")\n",
        "\n",
        "      else: # HERE\n",
        "      # Create a new CSV file IF path doesn't exist\n",
        "        # This check is no longer necessary as we generate a new id and path every time\n",
        "        # path_check = pd.read_csv(log_file)['model_name'].values # model_name\n",
        "\n",
        "        # if log['model_name'][0] not in path_check:\n",
        "        client.confirm_save(log['path'][0])\n",
        "        log.to_csv(log_file, mode='a', header=False, index=False)\n",
        "        print(f\"name: {log['model_name'][0]} \")\n",
        "        print(f\"path: {log['path'][0]} \")\n",
        "        print(f\"Logged client {client_num} to {log_file}\")\n",
        "        # else :\n",
        "        #   print(\"Existing Log\")\n",
        "      del client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log.head()"
      ],
      "metadata": {
        "id": "chlxrFtLoISt",
        "outputId": "ff8573c7-b79c-4bb2-a911-d715b5e092fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   client_id     backbone                            model_name  \\\n",
              "0         36  dino_vits16  5d33ee02-2a9b-473c-aa5e-b560a04af9e9   \n",
              "\n",
              "                     initial_model_name  \\\n",
              "0  b8437215-ee1e-48a4-8a5c-1fbfabaf6dcb   \n",
              "\n",
              "                                                path  num_of_clients  \\\n",
              "0  /content/drive/MyDrive/MLDL_FederatedLearning/...              80   \n",
              "\n",
              "       Measurement_criteria   accuracy      loss  train_loss  ...  \\\n",
              "0  accuracy,loss,train_loss  17.577197  3.877886     3.50951  ...   \n",
              "\n",
              "  client_test_size             train_test_ratio  classes  round_number  \\\n",
              "0              106  {'train': 0.8, 'test': 0.2}      all             1   \n",
              "\n",
              "    duration                 time  \\\n",
              "0  19.394295  2025-07-06 14:52:58   \n",
              "\n",
              "                                     path_to_subsets  \\\n",
              "0  /content/drive/MyDrive/MLDL_FederatedLearning/...   \n",
              "\n",
              "                                 path_to_class_combs  has_model_edited  \\\n",
              "0  /content/drive/MyDrive/MLDL_FederatedLearning/...              True   \n",
              "\n",
              "                                        weak_classes  \n",
              "0  [43, 69, 21, 3, 56, 95, 81, 65, 72, 29, 10, 55...  \n",
              "\n",
              "[1 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-068ec2b1-a811-413e-930f-afdedf50e871\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>client_id</th>\n",
              "      <th>backbone</th>\n",
              "      <th>model_name</th>\n",
              "      <th>initial_model_name</th>\n",
              "      <th>path</th>\n",
              "      <th>num_of_clients</th>\n",
              "      <th>Measurement_criteria</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>...</th>\n",
              "      <th>client_test_size</th>\n",
              "      <th>train_test_ratio</th>\n",
              "      <th>classes</th>\n",
              "      <th>round_number</th>\n",
              "      <th>duration</th>\n",
              "      <th>time</th>\n",
              "      <th>path_to_subsets</th>\n",
              "      <th>path_to_class_combs</th>\n",
              "      <th>has_model_edited</th>\n",
              "      <th>weak_classes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>36</td>\n",
              "      <td>dino_vits16</td>\n",
              "      <td>5d33ee02-2a9b-473c-aa5e-b560a04af9e9</td>\n",
              "      <td>b8437215-ee1e-48a4-8a5c-1fbfabaf6dcb</td>\n",
              "      <td>/content/drive/MyDrive/MLDL_FederatedLearning/...</td>\n",
              "      <td>80</td>\n",
              "      <td>accuracy,loss,train_loss</td>\n",
              "      <td>17.577197</td>\n",
              "      <td>3.877886</td>\n",
              "      <td>3.50951</td>\n",
              "      <td>...</td>\n",
              "      <td>106</td>\n",
              "      <td>{'train': 0.8, 'test': 0.2}</td>\n",
              "      <td>all</td>\n",
              "      <td>1</td>\n",
              "      <td>19.394295</td>\n",
              "      <td>2025-07-06 14:52:58</td>\n",
              "      <td>/content/drive/MyDrive/MLDL_FederatedLearning/...</td>\n",
              "      <td>/content/drive/MyDrive/MLDL_FederatedLearning/...</td>\n",
              "      <td>True</td>\n",
              "      <td>[43, 69, 21, 3, 56, 95, 81, 65, 72, 29, 10, 55...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 25 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-068ec2b1-a811-413e-930f-afdedf50e871')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-068ec2b1-a811-413e-930f-afdedf50e871 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-068ec2b1-a811-413e-930f-afdedf50e871');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "log"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load your DataFrame\n",
        "df = pd.read_csv('/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv')  # Change to your file path and format\n",
        "\n",
        "# Drop rows where 'path' is NaN or not a string\n",
        "df = df[df['path'].notna()]\n",
        "df['path'] = df['path'].astype(str).str.strip()  # Remove whitespace\n",
        "\n",
        "# Check if each path exists\n",
        "df_filtered = df[df['path'].apply(lambda x: os.path.exists(x))]\n",
        "n_error = len(df[\"client_id\"].values) -  len(df_filtered[\"client_id\"].values)\n",
        "print(f\" {n_error} clients' log are removed due to the not existing saved model file\")\n",
        "# Save the cleaned DataFrame\n",
        "df_filtered.to_csv('/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv', index=False)\n",
        "# print(len(df),len(df_filtered))"
      ],
      "metadata": {
        "id": "_FtdurVygZ__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import os\n",
        "\n",
        "# # Load your DataFrame\n",
        "# temp_df = pd.read_csv('/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv')  # Change to your file path and format\n",
        "# temp_df = temp_df[temp_df[\"has_model_edited\"]==False]\n",
        "\n",
        "# temp_df.to_csv('/content/drive/MyDrive/MLDL_FederatedLearning/csv/client_log.csv', index=False)"
      ],
      "metadata": {
        "id": "neAXr90vWuCb"
      },
      "execution_count": 25,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fu0Iz0LDlRDK",
        "bVtH-qLIrAeh"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}